import sys, os
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
import pandas as pd
from pyspark.sql.functions import expr
from bkfs.prod_parquets import ProdParquets
from bkfs.utils import setup_logger, get_status_df, need_current_run, upload_to_s3
from bkfs.mail import MailHandler
from datetime import datetime


logger = setup_logger()
if __name__ == '__main__':
## @params: [JOB_NAME]
    args = getResolvedOptions(sys.argv,  ['JOB_NAME', 'run_date', 'bucket_name',
                                      's3_path', 'email', 'output_addresses_name', 'output_names_name','run_assesment'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    logger.info(f'Arguments found:{args}')
    config = {
                'date': datetime.strptime(args['run_date'], '%Y%m%d').date(),
                'bucket_name': args['bucket_name'],
                'folder_name': args['s3_path'],
                'output_addresses_name': args['output_addresses_name'],
                'output_names_name': args['output_names_name'],
                'address_table_name': 'bks_lookup_unique_addresses_melissa_output',
                'names_table_name': 'bks_lookup_unique_names_melissa_output',
                'address_columns': ['ADR_LN1_V', 'CITY_V', 'STATE', 'ZIP_V', 'ZIP4_V'],
                'names_columns': ['M_FULLNAME'],
                'database': 'bkfs_automation',
                'proofpoint_server': '10.52.220.18',
                'mail_port': '25',
                'sender': 'bkfs_automation@verisk.com',
                'receiver': args['email'].split(',')
            }
    # logger.info(f'configs found:{config}')
    me = MailHandler(config)
    df = get_status_df(config['bucket_name'],
                   config['folder_name'], config['date'])
    prev_step = ['ADDRESS_SCRUBBING', 'NAMES_SCRUBBING']
    curr_step = 'PROD_ASSESSMENT'
    is_run_required = need_current_run(df, config['date'], prev_step, curr_step, False)
    is_run_required = True
    if not is_run_required:
        logger.info('Step already executed, so exiting')
    else:
        start = datetime.now()
        logger.info('Loading address table')
        addresses_df = glueContext.create_dynamic_frame.from_catalog(database='bkfs_automation', table_name='bks_lookup_unique_addresses_melissa_output').toDF()
        logger.info('Loaded address table')
        logger.info('Loading names table')
        names_output = glueContext.create_dynamic_frame.from_catalog(database='bkfs_automation', table_name='bks_lookup_unique_names_melissa_output').toDF()
        logger.info('Loaded names table')
        s3_path_names = f"s3://{config['bucket_name']}/{config['folder_name']}/names/preprocessed/{config['output_names_name']}_{config['date'].strftime('%Y%m%d')}.parquet"
        s3_path_addresses = f"s3://{config['bucket_name']}/{config['folder_name']}/addresses/preprocessed/{config['output_addresses_name']}_{config['date'].strftime('%Y%m%d')}.parquet"
        logger.info(f'Loading names pre processed table from path:{s3_path_names}')
        pre_names = spark.read.parquet(s3_path_names)
        logger.info(f'Loaded names pre processed table')
        logger.info(f'Loading addresses pre processed table from path:{s3_path_addresses}')
        pre_addr = spark.read.parquet(s3_path_addresses)
        logger.info(f'Loaded addresses pre processed table')
        logger.info('Extracting required addresses for run')
        addresses_df = addresses_df.alias('ua').join(pre_addr.alias('pa'), expr("""ua.ADR_LN1_V=pa.ADR_LN1_V and ua.CITY_V=pa.CITY_V and ua.STATE=pa.STATE and ua.ZIP_V=pa.ZIP_V and ua.ZIP4_V=pa.ZIP4_V"""), how='inner').selectExpr('ua.AddressKey','ua.ADR_LN1_V','ua.CITY_V','ua.STATE','ua.ZIP_V','ua.ZIP4_V')
        logger.info('Extracted required addresses for run')
        logger.info('Extracting required names for run')
        names_output = names_output.alias('un').join(pre_names.alias('pn'), expr("""un.M_FullName=pn.M_FullName"""),how='inner').selectExpr('un.M_FullName','un.subjectkey')
        logger.info('Extracted required names for run')
        addresses_df = addresses_df.fillna("")
        names_output = names_output.fillna("")
        newRow = spark.createDataFrame([("",-1)], ["M_FullName","subjectkey"])
        names_output=names_output.union(newRow)
        prod = ProdParquets(config,sc,spark,glueContext,job,F)
        prod_status = []
        try:
            pattren='Assessment_Update'
            logger.info(f"Started Generating Prod_{pattren} parquets")
            path =f"s3://{config['bucket_name']}/{config['folder_name']}/raw_parquet_files/{pattren}/Raw_{pattren}_{config['date'].strftime('%Y%m%d')}.parquet"
            logger.info(f'Reading raw parquet file from path: {path}')
            pd_raw_aspu = pd.read_parquet(path).fillna("")
            logger.info(f'completed reading raw parquet file')
            if(len(pd_raw_aspu)>0):
                logger.info(f'Data found to process {pattren}')
                raw_aspu = spark.read.parquet(path)
                s3_path = f"s3://{config['bucket_name']}/{config['folder_name']}/prod_parquet_files/{pattren}/prod_{pattren.lower()}_{config['date'].strftime('%Y%m%d')}.parquet"
                logger.info('Loaded data into spark df')
                logger.info("started Generating Assesment Parquets")
                final_df_aspu = prod.load_prod_assesment_update_prod(raw_aspu, addresses_df, names_output)
                logger.info("completed Generating Assesment Parquets")
                df = final_df_aspu.toPandas()
                prod.compare_raw_prod(pattren, pd_raw_aspu, df, prod_status)
                df['date_insert'] =  datetime.strptime(config['date'].strftime('%Y%m%d'), '%Y%m%d').date()
                logger.info("comparision was sucessfull")
                logger.info(f'Writing data to s3path:{s3_path}')
                df.to_parquet(s3_path)
                logger.info('Process completed')


            pattren='Assessment_Refresh'   
            logger.info(f"Started Generating Prod_{pattren} parquets")
            path = f"s3://{config['bucket_name']}/{config['folder_name']}/raw_parquet_files/{pattren}/Raw_{pattren}_{config['date'].strftime('%Y%m%d')}.parquet"
            logger.info(f'Reading raw parquet file from path: {path}')
            pd_raw_aspr = pd.read_parquet(path).fillna("")
            logger.info(f'completed reading raw parquet file')
            if(len(pd_raw_aspr))>0:
                logger.info(f'Data found to process {pattren}')
                raw_aspr = spark.read.parquet(path)
                s3_path = f"s3://{config['bucket_name']}/{config['folder_name']}/prod_parquet_files/{pattren}/prod_{pattren.lower()}_{config['date'].strftime('%Y%m%d')}.parquet"
                logger.info('Loaded data into spark df')
                logger.info("started Generating Assesment Parquets")
                final_df_aspr = prod.load_prod_assesment_refresh_prod(raw_aspr, addresses_df, names_output)
                logger.info("completed Generating Assesment Parquets")
                df = final_df_aspr.toPandas()
                prod.compare_raw_prod(pattren, pd_raw_aspr, df, prod_status)
                logger.info("comparision was sucessfull")
                df['date_insert'] =  datetime.strptime(config['date'].strftime('%Y%m%d'), '%Y%m%d').date()
                logger.info("comparision was sucessfull")
                logger.info(f'Writing data to s3path:{s3_path}')
                df.to_parquet(s3_path)
            
            
            df = pd.DataFrame.from_dict(prod_status)
            df['run_date'] = config['date']
            df=df.rename(columns = {'Prod Count':'actual_count', 'Raw Count':'source_count','Source Count':'source_count','Actual Count':'actual_count','File Name':'filename','Filename':'filename'})
            df.to_parquet(config['date'].strftime('%Y%m%d')+'_prod_assessment.parquet')
            logger.info(f'Stats after processing: {prod_status}')
            logger.info('Uploading stats to s3')
            upload_to_s3(config['bucket_name'], config['folder_name']+'/counts/'+config['date'].strftime(
                '%Y%m%d')+'_prod_assessment.parquet', config['date'].strftime('%Y%m%d')+'_prod_assessment.parquet')
            logger.info('Uploaded stats to s3')
            end = datetime.now()
            new_row = {
                'step_name': curr_step,
                'run_date': pd.to_datetime(config['date']),
                'start_date': pd.to_datetime(start),
                'end_date': pd.to_datetime(end),
                'status': 'Success',
                'message': 'Completed without errors'
            }
            new_df = pd.DataFrame([new_row])
            df = get_status_df(config['bucket_name'],
                               config['folder_name'], config['date'])
            df = pd.concat([df, new_df], ignore_index=True)
            path = 's3://' + config['bucket_name'] + '/' + config['folder_name']+'/status/' + \
                args['run_date'] + '_status.parquet'
            logger.info(f"status updating to s3: {path}")
            df.to_parquet(path)
            logger.info(f"status updated to s3: {path}")
        except Exception as e:
            logger.error(str(e))
            me.send_error_email(
                "Error in generating assessment prod parquet", "Error while generating assessment prod parquet. Please check logs for more details")
            raise e
    
job.commit()