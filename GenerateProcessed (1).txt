import sys
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import smtplib
import os
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
import pandas as pd
from pyspark.sql.functions import expr
import logging
import boto3
from datetime import datetime


def setup_logger():
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    log_format = logging.Formatter(
        '[%(levelname)s] %(filename)s %(asctime)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    cloudwatch_handler = logging.StreamHandler()
    cloudwatch_handler.setFormatter(log_format)
    logger.addHandler(cloudwatch_handler)
    # for handler in logger.handlers:
    #     handler.setFormatter(log_format)
    return logger


logger = setup_logger()


class MailHandler:
    def __init__(self, configs) -> None:
        self.proofpoint_server = configs['proofpoint_server']
        self.port = configs['mail_port']
        self.sender = configs['sender']
        self.receivers = configs['receiver']

    def send_completed_email(self, Subject, body):
        '''
        Function to send email notification to the reciepients mentioned.
        '''
        msg = MIMEMultipart()
        msg['Subject'] = Subject
        msg['From'] = self.sender
        msg['To'] = ','.join(self.receivers)
        html_body = MIMEText(body, 'html')
        msg.attach(html_body)
        logger.info(
            f'trying to send email to :f{self.receivers} using server: {self.proofpoint_server}:{self.port}\n with subject:{Subject}\n body:{html_body}')
        try:
            mailserver = smtplib.SMTP(self.proofpoint_server, self.port)
            mailserver.ehlo()
            mailserver.starttls()
            mailserver.ehlo()
            mailserver.sendmail(self.sender, self.receivers, msg.as_string())
            logger.info("Successfully sent email")
            mailserver.quit()
        except Exception as e:
            logger.error("Error: unable to send email" + str(e))
            raise e

    def send_error_email(self, step_name, error_msg):
        cont = f'''Hi Team,
        There is an error at step: {step_name}.
        Error: {error_msg}
        '''
        self.send_completed_email(
            "Error while processing bkfs automation job", cont)


def upload_to_s3(s3_bucket, s3_path, local_path):
    try:
        session = boto3.Session()
        S3Client = session.client('s3')
        status = S3Client.upload_file(local_path, s3_bucket, s3_path)
    except Exception as e:
        logger.error('Error uploading file', e)
        raise e
    return status


def get_status_df(bucket_name, s3_path, run_date):
    try:
        s3 = boto3.client('s3')
        path = s3_path+'/status/'+run_date+'_status.parquet'
        logger.info(f'downloading status file from s3 path:{path}')
        df = pd.read_parquet('s3://'+bucket_name+'/'+path)
        logger.info('downloaded status file from s3')
        return df
    except Exception as e:
        schema = {'step_name': 'str', 'run_date': 'datetime64[ns]', 'start_date': 'datetime64[ns]',
                  'end_date': 'datetime64[ns]', 'status': 'str', 'message': 'str'}
        logger.warn(
            'Failed to download file from s3 so returning empty dataframe')
        df = pd.DataFrame(columns=schema.keys()).astype(schema)
        return df


def need_current_run(df, run_date, prev_step, curr_step, is_first=False):
    lis = []
    if type(prev_step) == str:
        return _need_current_run(df, run_date, prev_step, curr_step, is_first)
    else:
        for prev in prev_step:
            lis.append(_need_current_run(
                df, run_date, prev, curr_step, is_first))
        flag = all(lis)
        if flag:
            logger.info('current step requires execution')
        else:
            logger.info(
                'skipping as all previous steps and current step are executed')
        return flag


def _need_current_run(df, run_date, prev_step, curr_step, is_first=False):
    logger.info(
        f"Checking if current run is required with previous step: {prev_step}")
    filter_date = pd.to_datetime(datetime.strptime(run_date, '%Y%m%d'))
    logger.info(filter_date)
    pre_filtered_df = df[(df['run_date'] == filter_date) & (
        df['step_name'] == prev_step) & (df['status'] == 'Success')]
    curr_filtered_df = df[(df['run_date'] == filter_date) & (
        df['step_name'] == curr_step) & (df['status'] == 'Success')]
    if len(pre_filtered_df) > 0 and len(curr_filtered_df) > 0:
        logger.info('Current step and previous step are already executed')
        return False
    if (len(pre_filtered_df) > 0 or is_first) and len(curr_filtered_df) == 0:
        logger.info(
            'previous step is already executed and current step is not executed')
        return True
    if len(pre_filtered_df) == 0:
        logger.error(f'Previous step:{prev_step} is not in completed state')
        raise Exception(f'Previous step:{prev_step} is not in completed state')


class GenerateNewParquet:

    def __init__(self, config) -> None:
        self.config = config
        self.date = config['date']
        self.bucket_name = config['bucket_name']
        self.folder_name = config['folder_name']
        self.output_addresses_name = config['output_addresses_name']
        self.output_names_name = config['output_names_name']

    def initialize_filesystem(self, s3bucketpath):
        URI = sc._gateway.jvm.java.net.URI
        FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem
        filesystem = FileSystem.get(
            URI(s3bucketpath), sc._jsc.hadoopConfiguration())
        return filesystem

    def read_parquet(self, source_path):
        logger.info(f"started reading data from:{source_path}")
        df = spark.read.parquet(source_path)
        logger.info(f"completed reading data from:{source_path}")
        return df

    def copy_s3_bucket(self, df, destination_path, partitions):
        logger.info(
            f'Exporting to :{destination_path} partitions:{partitions}')
        df.repartition(partitions).write.mode(
            'append').parquet(destination_path)
        logger.info(f'Exported to :{destination_path} partitions:{partitions}')
        return 1

    def rename_parquet(self, s3_path, name, date):
        logger.info(f"s3 path in rename parquet : {s3_path}")
        Path = sc._gateway.jvm.org.apache.hadoop.fs.Path
        fs = self.initialize_filesystem(s3_path)
        created_file_path = fs.globStatus(
            Path(s3_path + "part*.parquet"))[0].getPath()
        desti_path = f"{s3_path}/{name}_{date}.parquet"
        fs.rename(created_file_path, Path(desti_path))
        return 1

    def generate_address_parquet(self):
        addresses_source_path = str('/'.join(('s3:/', self.bucket_name, self.folder_name,
                                    'addresses/preprocessed', str(self.output_addresses_name+'_'+self.date+'.parquet'))))
        address_destination_path = '/'.join(
            ('s3:/', self.bucket_name, self.folder_name, 'addresses/processed/'))
        logger.info('Loaded previous address table ')
        melisa_address_main = glueContext.create_dynamic_frame.from_catalog(database=self.config["database"],
                                                                            table_name=self.config["address_table_name"]).toDF()
        last_add_key = melisa_address_main.agg(
            F.max('AddressKey')).collect()[0][0] + 1
        logger.info(f'Max address key found :{last_add_key}')
        logger.info(
            f'Loading preprocessed address from: {addresses_source_path}')
        melisa_address_temp = self.read_parquet(addresses_source_path)
        logger.info(
            f'Loaded preprocessed address from: {addresses_source_path}')
        unique_records_address = melisa_address_temp.join(melisa_address_main, on=self.config["address_columns"],
                                                          how="left_anti")
        logger.info(f'Prepared data required for scrubbing and writing to: {address_destination_path}')
        unique_records_address = unique_records_address.withColumn(
            'AddressKey', F.monotonically_increasing_id() + last_add_key)
        status = self.copy_s3_bucket(
            unique_records_address, address_destination_path, 1)
        logger.info('Exported data to s3')
        status = self.rename_parquet(
            address_destination_path, self.output_addresses_name, self.date)
        logger.info('Renaming completed')
        # return unique_records_address.count()

    def generate_names_parquet(self): 
        names_source_path = str('/'.join(('s3:/', self.bucket_name, self.folder_name,
                                'names/preprocessed', str(self.output_names_name+'_'+self.date+'.parquet'))))
        names_destination_path = str(
            '/'.join(('s3:/', self.bucket_name, self.folder_name, 'names/processed/')))
        logger.info('Loaded previous names table ')
        melisa_names_main = glueContext.create_dynamic_frame.from_catalog(database=self.config["database"],
                                                                          table_name=self.config["names_table_name"]).toDF()
        last_sub_key = melisa_names_main.agg(
            F.max('SubjectKey')).collect()[0][0] + 1
        logger.info(f'Max subject key found :{last_sub_key}')
        logger.info(
            f'Loading preprocessed names from: {names_source_path}')
        melisa_names_temp = self.read_parquet(names_source_path)
        logger.info(
            f'Loaded preprocessed names from: {names_source_path}')
        unique_records_names = melisa_names_temp.join(melisa_names_main, on=self.config["names_columns"],
                                                      how="left_anti")
        logger.info(f'Prepared data required for scrubbing and writing to: {names_destination_path}')
        unique_records_names = unique_records_names.withColumn(
            'SubjectKey', F.monotonically_increasing_id() + last_sub_key)
        status = self.copy_s3_bucket(
            unique_records_names, names_destination_path, 1)
        logger.info('Exported data to s3')
        status = self.rename_parquet(
            names_destination_path, self.output_names_name, self.date)
        logger.info('Renaming completed')
        # return unique_records_names.count()


if __name__ == '__main__':
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'run_date', 'bucket_name',
                                  's3_path', 'email', 'output_addresses_name', 'output_names_name'])
    logger.info(f'Arguments found:{args}')
    config = {
            'date': args['run_date'],
            'bucket_name': args['bucket_name'],
            'folder_name': args['s3_path'],
            'output_addresses_name': args['output_addresses_name'],
            'output_names_name': args['output_names_name'],
            'address_table_name': 'bks_lookup_unique_addresses_melissa_output',
            'names_table_name': 'bks_lookup_unique_names_melissa_output',
            'address_columns': ['ADR_LN1_V', 'CITY_V', 'STATE', 'ZIP_V', 'ZIP4_V'],
            'names_columns': ['M_FULLNAME'],
            'database': 'bkfs_automation',
            'proofpoint_server': '10.52.220.18',
            'mail_port': '25',
            'sender': 'bkfs_automation@verisk.com',
            'receiver': args['email'].split(',')
            
        }
    me = MailHandler(config)
    
    df = get_status_df(config['bucket_name'],
                      config['folder_name'], config['date'])
    prev_step = 'UNI_NAMES_ADDR'
    curr_step = 'PROCESS_NAMES_ADDR'
    is_run_required = need_current_run(
        df, config['date'], prev_step, curr_step, False)
    if not is_run_required:
        logger.info('Step already executed, so exiting')
    else:
        try:
            start = datetime.now()
            sc = SparkContext()
            glueContext = GlueContext(sc)
            spark = glueContext.spark_session
            job = Job(glueContext)
            job.init(args['JOB_NAME'], args)
            logger.info('Initialised glue')
            logger.info(f'config:{config}')
            obj = GenerateNewParquet(config)
            logger.info('Started extracting names for scrubbing')
            obj.generate_names_parquet()
            logger.info('Completed extracting names for scrubbing')
            logger.info('Started extracting addresses for scrubbing')
            obj.generate_address_parquet()
            logger.info('Completed extracting addresses for scrubbing')
            job.commit()
            cou_path = "s3://" + config['bucket_name'] + '/' + config['folder_name'] + '/counts/' + config['date'] + '_names_addr.parquet'
            logger.info(
                f'Started loading data for counts from path: {cou_path}')
            cou_df = pd.read_parquet(cou_path)
            logger.info(f'Completed loading data for counts df')
            names_path ="s3://" + config['bucket_name'] + '/' + config['folder_name'] + '/names/processed/' + config['output_names_name'] + '_' + config['date'] + '.parquet'
            address_path ="s3://" + config['bucket_name'] + '/' + config['folder_name'] + '/addresses/processed/' + config['output_addresses_name'] + '_' + config['date'] + '.parquet'
            names_df = pd.read_parquet(names_path)
            addresse_df = pd.read_parquet(address_path)
            cou_df.loc[(cou_df['run_date'] == datetime.strptime(config['date'], '%Y%m%d')) & (cou_df['File Name'] == 'Names'), 'Actual Count'] = len(names_df)
            cou_df.loc[(cou_df['run_date'] == datetime.strptime(config['date'], '%Y%m%d')) & (cou_df['File Name'] == 'Addresses'), 'Actual Count'] = len(addresse_df)
            cou_df=cou_df.rename(columns = {'Prod Count':'actual_count', 'Raw Count':'source_count','Source Count':'source_count','Actual Count':'actual_count','File Name':'filename','Filename':'filename'})
            cou_df['run_date'] = datetime.strptime(config['date'], '%Y%m%d').date()
            cou_df.to_parquet(config['date']+'_names_addr.parquet')
            logger.info(f'Stats after processing: {cou_df}')
            logger.info('Uploading stats to s3')
            upload_to_s3(config['bucket_name'], config['folder_name']+'/counts/'+config['date']+'_names_addr.parquet', config['date']+'_names_addr.parquet')
            logger.info('Uploaded stats to s3')
            end = datetime.now()
            new_row = {
                'step_name': curr_step,
                'run_date': pd.to_datetime(datetime.strptime(config['date'], '%Y%m%d')),
                'start_date': pd.to_datetime(start),
                'end_date': pd.to_datetime(end),
                'status': 'Success',
                'message': 'Completed without errors'
            }
            new_df = pd.DataFrame([new_row])
            df = get_status_df(config['bucket_name'],
                              config['folder_name'], config['date'])
            df = pd.concat([df, new_df], ignore_index=True)
            df.to_parquet('status.parquet')
            path = args['s3_path']+'/status/' + \
                config['date']+'_status.parquet'
            upload_to_s3(config['bucket_name'], path, 'status.parquet')
            logger.info("status updated")
        except Exception as e:
            logger.error(e)
            me.send_error_email(
                "Error in extracting new names and addresses generation", "Error while extracting new names and addresses generation. Please check logs for more details")
            raise e
